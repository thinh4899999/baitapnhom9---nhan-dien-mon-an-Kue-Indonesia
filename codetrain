!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ilhamfp31/kue-indonesia
!unzip kue-indonesia.zip -d kue-indonesia

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

print(f"TensorFlow version: {tf.__version__}")
print(f"GPU Available: {tf.config.list_physical_devices('GPU')}")

# =============================================================================
# B∆Ø·ªöC 1: C·∫§U H√åNH V√Ä THAM S·ªê T·ªêI √ÅT
# =============================================================================

# Tham s·ªë t·ªëi ∆∞u
IMG_SIZE = 224  # K√≠ch th∆∞·ªõc t·ªëi ∆∞u cho EfficientNet
BATCH_SIZE = 16  # Gi·∫£m batch size ƒë·ªÉ tr√°nh OOM
EPOCHS = 30
LEARNING_RATE = 0.001

data_dir = '/content/kue-indonesia'

# =============================================================================
# B∆Ø·ªöC 2: DATA AUGMENTATION TH√îNG MINH
# =============================================================================

# Data augmentation c√¢n b·∫±ng - kh√¥ng qu√° m·∫°nh
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,          # Gi·∫£m rotation ƒë·ªÉ tr√°nh bi·∫øn d·∫°ng
    width_shift_range=0.2,      # Gi·∫£m shift
    height_shift_range=0.2,
    zoom_range=0.15,            # Gi·∫£m zoom
    horizontal_flip=True,
    brightness_range=[0.9, 1.1], # Gi·∫£m ƒë·ªô thay ƒë·ªïi brightness
    fill_mode='nearest'
)

# Validation kh√¥ng augmentation
val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# =============================================================================
# B∆Ø·ªöC 3: T·∫¢I D·ªÆ LI·ªÜU
# =============================================================================

train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = val_datagen.flow_from_directory(
    data_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

num_classes = len(train_generator.class_indices)
print(f'S·ªë l∆∞·ª£ng classes: {num_classes}')
print(f'Classes: {train_generator.class_indices}')
print(f'Training samples: {train_generator.samples}')
print(f'Validation samples: {val_generator.samples}')

# =============================================================================
# B∆Ø·ªöC 4: X√ÇY D·ª∞NG M√î H√åNH TRANSFER LEARNING T·ªêI ∆ØU
# =============================================================================

def create_optimized_model(num_classes):
    """T·∫°o m√¥ h√¨nh t·ªëi ∆∞u s·ª≠ d·ª•ng EfficientNetB0"""
    
    # Base model v·ªõi weights t·ª´ ImageNet
    base_model = EfficientNetB0(
        weights='imagenet',
        include_top=False,
        input_shape=(IMG_SIZE, IMG_SIZE, 3)
    )
    
    # Freeze base model ban ƒë·∫ßu
    base_model.trainable = False
    
    # X√¢y d·ª±ng model
    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    
    return model, base_model

# T·∫°o model
model, base_model = create_optimized_model(num_classes)

# Compile model
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nüìä KI·∫æN TR√öC M√î H√åNH:")
model.summary()

# =============================================================================
# B∆Ø·ªöC 5: CALLBACKS T·ªêI ∆ØU
# =============================================================================

callbacks = [
    EarlyStopping(
        monitor='val_accuracy',
        patience=8,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.3,
        patience=4,
        min_lr=1e-7,
        verbose=1
    ),
    ModelCheckpoint(
        'best_kue_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]

# =============================================================================
# B∆Ø·ªöC 6: TRAINING GIAI ƒêO·∫†N 1 - FROZEN BASE
# =============================================================================

print("\nüöÄ GIAI ƒêO·∫†N 1: Training v·ªõi base model b·ªã freeze")
print("="*60)

history1 = model.fit(
    train_generator,
    epochs=15,
    validation_data=val_generator,
    callbacks=callbacks,
    verbose=1
)

# =============================================================================
# B∆Ø·ªöC 7: FINE-TUNING - UNFREEZE M·ªòT S·ªê LAYERS
# =============================================================================

print("\nüî• GIAI ƒêO·∫†N 2: Fine-tuning - Unfreeze top layers")
print("="*60)

# Unfreeze top layers c·ªßa base model
base_model.trainable = True

# Freeze c√°c layer ƒë·∫ßu, ch·ªâ fine-tune top layers
for layer in base_model.layers[:-20]:
    layer.trainable = False

# Compile l·∫°i v·ªõi learning rate th·∫•p h∆°n cho fine-tuning
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE/10),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Continue training
history2 = model.fit(
    train_generator,
    epochs=15,
    validation_data=val_generator,
    callbacks=callbacks,
    verbose=1,
    initial_epoch=len(history1.history['accuracy'])
)

# =============================================================================
# B∆Ø·ªöC 8: K·∫æT H·ª¢P L·ªäCH S·ª¨ TRAINING
# =============================================================================

def combine_histories(hist1, hist2):
    """K·∫øt h·ª£p 2 history objects"""
    combined_history = {}
    for key in hist1.history.keys():
        combined_history[key] = hist1.history[key] + hist2.history[key]
    return combined_history

combined_history = combine_histories(history1, history2)

# =============================================================================
# B∆Ø·ªöC 9: ƒê√ÅNH GI√Å V√Ä VISUALIZE K·∫æT QU·∫¢
# =============================================================================

def plot_training_results(history):
    """V·∫Ω bi·ªÉu ƒë·ªì k·∫øt qu·∫£ training"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Accuracy
    axes[0,0].plot(history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)
    axes[0,0].plot(history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)
    axes[0,0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
    axes[0,0].set_xlabel('Epoch')
    axes[0,0].set_ylabel('Accuracy')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # Loss
    axes[0,1].plot(history['loss'], 'b-', label='Training Loss', linewidth=2)
    axes[0,1].plot(history['val_loss'], 'r-', label='Validation Loss', linewidth=2)
    axes[0,1].set_title('Model Loss', fontsize=14, fontweight='bold')
    axes[0,1].set_xlabel('Epoch')
    axes[0,1].set_ylabel('Loss')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)
    
    # Learning Rate (n·∫øu c√≥)
    if 'lr' in history:
        axes[1,0].plot(history['lr'], 'g-', linewidth=2)
        axes[1,0].set_title('Learning Rate', fontsize=14, fontweight='bold')
        axes[1,0].set_xlabel('Epoch')
        axes[1,0].set_ylabel('Learning Rate')
        axes[1,0].set_yscale('log')
        axes[1,0].grid(True, alpha=0.3)
    
    # Accuracy zoom
    axes[1,1].plot(history['accuracy'][-10:], 'b-', label='Training (Last 10)', linewidth=2)
    axes[1,1].plot(history['val_accuracy'][-10:], 'r-', label='Validation (Last 10)', linewidth=2)
    axes[1,1].set_title('Final Accuracy (Last 10 Epochs)', fontsize=14, fontweight='bold')
    axes[1,1].set_xlabel('Epoch')
    axes[1,1].set_ylabel('Accuracy')
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# V·∫Ω bi·ªÉu ƒë·ªì
plot_training_results(combined_history)

# =============================================================================
# B∆Ø·ªöC 10: ƒê√ÅNH GI√Å CHI TI·∫æT
# =============================================================================

print("\nüìä ƒê√ÅNH GI√Å M√î H√åNH:")
print("="*50)

# ƒê√°nh gi√° tr√™n validation set
val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)
print(f'Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)')
print(f'Validation Loss: {val_loss:.4f}')

# T·∫°o predictions cho confusion matrix
val_generator.reset()
predictions = model.predict(val_generator, verbose=1)
y_pred = np.argmax(predictions, axis=1)
y_true = val_generator.classes

# Class names
class_names = list(val_generator.class_indices.keys())

# Classification report
print(f"\nüìã CLASSIFICATION REPORT:")
print("="*50)
print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()

# =============================================================================
# B∆Ø·ªöC 11: H√ÄM TEST NHANH
# =============================================================================

def predict_single_image(image_path, model, class_names, show_image=True):
    """D·ª± ƒëo√°n m·ªôt ·∫£nh ƒë∆°n l·∫ª"""
    from tensorflow.keras.utils import load_img, img_to_array
    
    # Load v√† preprocess ·∫£nh
    img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))
    
    if show_image:
        plt.figure(figsize=(8, 6))
        plt.imshow(img)
        plt.axis('off')
        plt.title('Input Image', fontsize=14, fontweight='bold')
        plt.show()
    
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array / 255.0
    
    # Predict
    predictions = model.predict(img_array, verbose=0)
    predicted_class = np.argmax(predictions[0])
    confidence = np.max(predictions[0])
    
    print(f"\nüéØ K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:")
    print(f"M√≥n ƒÉn: {class_names[predicted_class]}")
    print(f"ƒê·ªô tin c·∫≠y: {confidence:.4f} ({confidence*100:.2f}%)")
    
    # Hi·ªÉn th·ªã top 3 predictions
    top_3_indices = np.argsort(predictions[0])[::-1][:3]
    print(f"\nüìä TOP 3 D·ª∞ ƒêO√ÅN:")
    for i, idx in enumerate(top_3_indices, 1):
        print(f"{i}. {class_names[idx]}: {predictions[0][idx]*100:.2f}%")
    
    return class_names[predicted_class], confidence

# =============================================================================
# B∆Ø·ªöC 12: L∆ØU M√î H√åNH
# =============================================================================

# L∆∞u model cu·ªëi c√πng
model.save('kue_classifier_final.h5')
print(f"\n‚úÖ M√î H√åNH ƒê√É ƒê∆Ø·ª¢C L∆ØU:")
print(f"üìÅ best_kue_model.h5 (best validation accuracy)")
print(f"üìÅ kue_classifier_final.h5 (final model)")

# Hi·ªÉn th·ªã th√¥ng tin t·ªïng k·∫øt
print(f"\nüéâ TRAINING HO√ÄN T·∫§T!")
print(f"üéØ Validation Accuracy: {val_accuracy*100:.2f}%")
print(f"üìä S·ªë classes: {num_classes}")
print(f"üî• Epochs trained: {len(combined_history['accuracy'])}")

# Test v·ªõi ·∫£nh m·∫´u (n·∫øu c√≥)
import os
sample_dirs = ['/content/kue-indonesia/' + class_name for class_name in class_names]
for sample_dir in sample_dirs:
    if os.path.exists(sample_dir):
        sample_files = [f for f in os.listdir(sample_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        if sample_files:
            sample_path = os.path.join(sample_dir, sample_files[0])
            print(f"\nüß™ TEST V·ªöI ·∫¢NH M·∫™U: {sample_path}")
            predict_single_image(sample_path, model, class_names)
            break

print(f"\nüí° S·ª¨ D·ª§NG MODEL:")
print(f"model = tf.keras.models.load_model('best_kue_model.h5')")
print(f"predict_single_image('path/to/image.jpg', model, {class_names})")
